{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class properties_of:\n",
    "    def __init__(self, name: str, engine: str = \"pandas\"):\n",
    "        \"\"\"\n",
    "        Inicializuojama klasė. Palaikomi du varikliai: 'pandas' ir 'pyspark'.\n",
    "\n",
    "        :param name: Objektų grupės pavadinimas.\n",
    "        :param engine: Variklis ('pandas' arba 'pyspark').\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.engine = engine\n",
    "\n",
    "        if engine == \"pandas\":\n",
    "            self.df_property = pd.DataFrame(columns=[\"id\", \"property_id\", \"value\"])\n",
    "            self.df_property_type = pd.DataFrame(columns=[\"property_id\", \"description\"])\n",
    "        elif engine == \"pyspark\":\n",
    "            from pyspark.sql import SparkSession\n",
    "            self.spark = SparkSession.builder.master(\"local\").appName(\"PropertiesDB\").getOrCreate()\n",
    "            self.df_property = self.spark.createDataFrame([], schema=\"id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property_type = self.spark.createDataFrame([], schema=\"property_id STRING, description STRING\")\n",
    "        else:\n",
    "            raise ValueError(\"Nepalaikomas variklis: pasirinkite 'pandas' arba 'pyspark'.\")\n",
    "\n",
    "    def add_property(self, id: str, property_id: str, value: str, check_property_type: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Pridedama savybė konkrečiam objektui.\n",
    "\n",
    "        :param id: Objekto ID.\n",
    "        :param property_id: Savybės ID.\n",
    "        :param value: Savybės reikšmė.\n",
    "        :param check_property_type: Tikrinti, ar savybės tipas egzistuoja.\n",
    "        \"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            if check_property_type and property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.df_property = pd.concat([\n",
    "                self.df_property,\n",
    "                pd.DataFrame({\"id\": [id], \"property_id\": [property_id], \"value\": [value]})\n",
    "            ], ignore_index=True).drop_duplicates(subset=[\"id\", \"property_id\"])\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if check_property_type:\n",
    "                existing = self.df_property_type.filter(f\"property_id = '{property_id}'\").count() > 0\n",
    "                if not existing:\n",
    "                    raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            new_row = self.spark.createDataFrame([(id, property_id, value)], schema=\"id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property = self.df_property.union(new_row)\n",
    "\n",
    "    def export_to_narrow(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Eksportuoja savybes į siauro formato Pandas DataFrame.\n",
    "\n",
    "        :return: Pandas DataFrame su stulpeliais ['id', 'property_id', 'value'].\n",
    "        \"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            return self.df_property.copy()\n",
    "        elif self.engine == \"pyspark\":\n",
    "            return self.df_property.toPandas()\n",
    "\n",
    "    def import_from_narrow(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Importuoja siauro formato Pandas DataFrame į duomenų bazę.\n",
    "\n",
    "        :param df: Pandas DataFrame su stulpeliais ['id', 'property_id', 'value'].\n",
    "        \"\"\"\n",
    "        for _, row in df.iterrows():\n",
    "            self.add_property(row[\"id\"], row[\"property_id\"], row[\"value\"])\n",
    "\n",
    "    def export_to_wide(self, properties: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Eksportuoja savybes į platų Pandas DataFrame.\n",
    "\n",
    "        :param properties: Savybių sąrašas. Jei `None`, grąžina visas savybes.\n",
    "        :return: Pandas DataFrame su kiekvienai savybei atskiru stulpeliu.\n",
    "        \"\"\"\n",
    "        df_narrow = self.export_to_narrow()\n",
    "        df = df_narrow.pivot(index=\"id\", columns=\"property_id\", values=\"value\").reset_index()\n",
    "\n",
    "        if properties:\n",
    "            all_columns = [\"id\"] + properties\n",
    "            df = df.reindex(columns=all_columns, fill_value=None)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def import_from_wide(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Importuoja savybes iš plataus Pandas DataFrame.\n",
    "\n",
    "        :param df: Pandas DataFrame su stulpeliais: `id`, savybės ir jų reikšmės.\n",
    "        \"\"\"\n",
    "        id_col = \"id\"\n",
    "        for _, row in df.iterrows():\n",
    "            for property_id, value in row.drop(labels=[id_col]).items():\n",
    "                if pd.notna(value):\n",
    "                    self.add_property(row[id_col], property_id, value)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Uždaromas PySpark sesija, jei naudojama.\"\"\"\n",
    "        if self.engine == \"pyspark\":\n",
    "            self.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siauras formatas:\n",
      "            id property_id               value\n",
      "0  111-222-333       title          Lapė Snapė\n",
      "1  111-222-333      author      Jojas Papievis\n",
      "2  111-222-333        year                2020\n",
      "3  222-333-444       title       Vilkas Pilkas\n",
      "4  222-333-444      author  Antanas Antanaitis\n",
      "5  222-333-444        year                1900\n",
      "Platus formatas:\n",
      "property_id           id              author          title  year\n",
      "0            111-222-333      Jojas Papievis     Lapė Snapė  2020\n",
      "1            222-333-444  Antanas Antanaitis  Vilkas Pilkas  1900\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    # Testavimas\n",
    "    obj = properties_of(\"knyga\", engine=\"pandas\")\n",
    "\n",
    "    # Test import_from_wide\n",
    "    data = pd.DataFrame({\n",
    "        \"id\": [\"111-222-333\", \"222-333-444\"],\n",
    "        \"title\": [\"Lapė Snapė\", \"Vilkas Pilkas\"],\n",
    "        \"author\": [\"Jojas Papievis\", \"Antanas Antanaitis\"],\n",
    "        \"year\": [\"2020\", \"1900\"]\n",
    "    })\n",
    "    obj.import_from_wide(data)\n",
    "\n",
    "    # Eksportavimas į siaurą lentelę\n",
    "    narrow = obj.export_to_narrow()\n",
    "    print(\"Siauras formatas:\")\n",
    "    print(narrow)\n",
    "\n",
    "    # Importavimas iš siauro formato\n",
    "    obj.import_from_narrow(narrow)\n",
    "\n",
    "    # Eksportas į platų formatą\n",
    "    print(\"Platus formatas:\")\n",
    "    print(obj.export_to_wide())\n",
    "\n",
    "    obj.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
