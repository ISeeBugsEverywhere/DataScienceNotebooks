{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlite3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame \u001b[38;5;28;01mas\u001b[39;00m PySparkDF\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mproperties_of\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame as PySparkDF\n",
    "\n",
    "\n",
    "class properties_of:\n",
    "    def __init__(self, name, db=\":memory:\"):\n",
    "        \"\"\"\n",
    "        Inicijuoja objektą su SQLite, Pandas DataFrame arba PySpark DataFrame pagrindu.\n",
    "\n",
    "        :param name: Objekto pavadinimas (naudojamas lentelių pavadinimams kurti).\n",
    "        :param db:\n",
    "            - \":memory:\" arba str: SQLite failo kelias arba atmintyje laikoma duomenų bazė.\n",
    "            - sqlite3.Connection: Egzistuojantis SQLite ryšys.\n",
    "            - pandas.DataFrame: Tuščias arba užpildytas Pandas DataFrame.\n",
    "            - pyspark.sql.DataFrame: Tuščias arba užpildytas PySpark DataFrame.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "\n",
    "        if isinstance(db, pd.DataFrame):\n",
    "            # Naudojamas Pandas DataFrame\n",
    "            self.db_type = \"dataframe\"\n",
    "            self.df_property = db if not db.empty else pd.DataFrame(columns=[\"object_id\", \"property_id\", \"value\"])\n",
    "            self.df_property_type = pd.DataFrame(columns=[\"property_id\", \"description\"])\n",
    "        elif isinstance(db, PySparkDF):\n",
    "            # Naudojamas PySpark DataFrame\n",
    "            self.db_type = \"pyspark\"\n",
    "            self.spark = SparkSession.builder.getOrCreate()\n",
    "            self.df_property = db if db else self.spark.createDataFrame([], schema=\"object_id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property_type = self.spark.createDataFrame([], schema=\"property_id STRING, description STRING\")\n",
    "        elif isinstance(db, sqlite3.Connection):\n",
    "            # Naudojamas esamas SQLite ryšys\n",
    "            self.db_type = \"sqlite\"\n",
    "            self.conn = db\n",
    "            self.cursor = self.conn.cursor()\n",
    "            self._create_tables()\n",
    "        else:\n",
    "            # Naudojamas SQLite failas arba \":memory:\"\n",
    "            self.db_type = \"sqlite\"\n",
    "            self.conn = sqlite3.connect(db)\n",
    "            self.cursor = self.conn.cursor()\n",
    "            self._create_tables()\n",
    "\n",
    "    def _create_tables(self):\n",
    "        \"\"\"Sukuriamos reikalingos lentelės SQLite duomenų bazėje.\"\"\"\n",
    "        self.cursor.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.name}_property (\n",
    "                object_id TEXT,\n",
    "                property_id TEXT,\n",
    "                value TEXT,\n",
    "                PRIMARY KEY (object_id, property_id)\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.cursor.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.name}_property_type (\n",
    "                property_id TEXT PRIMARY KEY,\n",
    "                description TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def add_property_type(self, property_id, description):\n",
    "        \"\"\"Pridedamas savybės tipas.\"\"\"\n",
    "        if self.db_type == \"dataframe\":\n",
    "            if property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                self.df_property_type = pd.concat([\n",
    "                    self.df_property_type,\n",
    "                    pd.DataFrame({\"property_id\": [property_id], \"description\": [description]})\n",
    "                ], ignore_index=True)\n",
    "        elif self.db_type == \"pyspark\":\n",
    "            if not self.df_property_type.filter(self.df_property_type.property_id == property_id).count():\n",
    "                new_row = self.spark.createDataFrame([(property_id, description)], schema=\"property_id STRING, description STRING\")\n",
    "                self.df_property_type = self.df_property_type.union(new_row)\n",
    "        else:\n",
    "            self.cursor.execute(f\"\"\"\n",
    "                INSERT OR IGNORE INTO {self.name}_property_type (property_id, description)\n",
    "                VALUES (?, ?)\n",
    "            \"\"\", (property_id, description))\n",
    "            self.conn.commit()\n",
    "\n",
    "    def add_property(self, object_id, property_id, value, check_property_type=False):\n",
    "        \"\"\"Pridedama savybė konkrečiam objektui.\"\"\"\n",
    "        if self.db_type == \"dataframe\":\n",
    "            if check_property_type and property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.df_property = pd.concat([\n",
    "                self.df_property,\n",
    "                pd.DataFrame({\"object_id\": [object_id], \"property_id\": [property_id], \"value\": [value]})\n",
    "            ], ignore_index=True).drop_duplicates(subset=[\"object_id\", \"property_id\"])\n",
    "        elif self.db_type == \"pyspark\":\n",
    "            if check_property_type and not self.df_property_type.filter(self.df_property_type.property_id == property_id).count():\n",
    "                raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            new_row = self.spark.createDataFrame([(object_id, property_id, value)], schema=\"object_id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property = self.df_property.union(new_row)\n",
    "        else:\n",
    "            if check_property_type:\n",
    "                self.cursor.execute(f\"\"\"\n",
    "                    SELECT 1 FROM {self.name}_property_type WHERE property_id = ?\n",
    "                \"\"\", (property_id,))\n",
    "                if not self.cursor.fetchone():\n",
    "                    raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.cursor.execute(f\"\"\"\n",
    "                INSERT OR REPLACE INTO {self.name}_property (object_id, property_id, value)\n",
    "                VALUES (?, ?, ?)\n",
    "            \"\"\", (object_id, property_id, value))\n",
    "            self.conn.commit()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Uždaromas ryšys su duomenų baze, jei naudojama SQLite.\"\"\"\n",
    "        if self.db_type == \"sqlite\":\n",
    "            self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite Test\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'properties_of' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     pyspark_test\u001b[38;5;241m.\u001b[39mdf_property\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# SQLite testavimas\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQLite Test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     sqlite_test \u001b[38;5;241m=\u001b[39m \u001b[43mproperties_of\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtestas\u001b[39m\u001b[38;5;124m\"\u001b[39m, db\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:memory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     sqlite_test\u001b[38;5;241m.\u001b[39madd_property_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVardas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     sqlite_test\u001b[38;5;241m.\u001b[39madd_property(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'properties_of' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # SQLite testavimas\n",
    "    print(\"SQLite Test\")\n",
    "    sqlite_test = properties_of(\"testas\", db=\":memory:\")\n",
    "    sqlite_test.add_property_type(\"name\", \"Vardas\")\n",
    "    sqlite_test.add_property(\"1\", \"name\", \"John\")\n",
    "    sqlite_test.close()\n",
    "\n",
    "    # Pandas DF testavimas\n",
    "    print(\"\\nPandas Test\")\n",
    "    pandas_test = properties_of(\"testas\", db=pd.DataFrame())\n",
    "    pandas_test.add_property_type(\"name\", \"Vardas\")\n",
    "    pandas_test.add_property(\"1\", \"name\", \"Jane\")\n",
    "    print(pandas_test.df_property)\n",
    "\n",
    "    # PySpark DF testavimas\n",
    "    print(\"\\nPySpark Test\")\n",
    "    spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "    pyspark_test = properties_of(\"testas\", db=spark.createDataFrame([], \"object_id STRING, property_id STRING, value STRING\"))\n",
    "    pyspark_test.add_property_type(\"name\", \"Vardas\")\n",
    "    pyspark_test.add_property(\"1\", \"name\", \"Doe\")\n",
    "    pyspark_test.df_property.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
