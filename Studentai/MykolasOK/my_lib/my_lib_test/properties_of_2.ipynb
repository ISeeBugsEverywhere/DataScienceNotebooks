{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame as PySparkDF\n",
    "\n",
    "\n",
    "class properties_of:\n",
    "    def __init__(self, name, db=\":memory:\"):\n",
    "        \"\"\"\n",
    "        Inicijuoja objektą su SQLite, Pandas DataFrame arba PySpark DataFrame pagrindu.\n",
    "\n",
    "        :param name: Objekto pavadinimas (naudojamas lentelių pavadinimams kurti).\n",
    "        :param db:\n",
    "            - \":memory:\" arba str: SQLite failo kelias arba atmintyje laikoma duomenų bazė.\n",
    "            - sqlite3.Connection: Egzistuojantis SQLite ryšys.\n",
    "            - pandas.DataFrame: Tuščias arba užpildytas Pandas DataFrame.\n",
    "            - pyspark.sql.DataFrame: Tuščias arba užpildytas PySpark DataFrame.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "\n",
    "        if isinstance(db, pd.DataFrame):\n",
    "            # Naudojamas Pandas DataFrame\n",
    "            self.db_type = \"dataframe\"\n",
    "            self.df_property = db if not db.empty else pd.DataFrame(columns=[\"object_id\", \"property_id\", \"value\"])\n",
    "            self.df_property_type = pd.DataFrame(columns=[\"property_id\", \"description\"])\n",
    "        elif isinstance(db, PySparkDF):\n",
    "            # Naudojamas PySpark DataFrame\n",
    "            self.db_type = \"pyspark\"\n",
    "            self.spark = SparkSession.builder.getOrCreate()\n",
    "            self.df_property = db if db else self.spark.createDataFrame([], schema=\"object_id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property_type = self.spark.createDataFrame([], schema=\"property_id STRING, description STRING\")\n",
    "        elif isinstance(db, sqlite3.Connection):\n",
    "            # Naudojamas esamas SQLite ryšys\n",
    "            self.db_type = \"sqlite\"\n",
    "            self.conn = db\n",
    "            self.cursor = self.conn.cursor()\n",
    "            self._create_tables()\n",
    "        else:\n",
    "            # Naudojamas SQLite failas arba \":memory:\"\n",
    "            self.db_type = \"sqlite\"\n",
    "            self.conn = sqlite3.connect(db)\n",
    "            self.cursor = self.conn.cursor()\n",
    "            self._create_tables()\n",
    "\n",
    "    def _create_tables(self):\n",
    "        \"\"\"Sukuriamos reikalingos lentelės SQLite duomenų bazėje.\"\"\"\n",
    "        self.cursor.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.name}_property (\n",
    "                object_id TEXT,\n",
    "                property_id TEXT,\n",
    "                value TEXT,\n",
    "                PRIMARY KEY (object_id, property_id)\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.cursor.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.name}_property_type (\n",
    "                property_id TEXT PRIMARY KEY,\n",
    "                description TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def add_property_type(self, property_id, description):\n",
    "        \"\"\"Pridedamas savybės tipas.\"\"\"\n",
    "        if self.db_type == \"dataframe\":\n",
    "            if property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                self.df_property_type = pd.concat([\n",
    "                    self.df_property_type,\n",
    "                    pd.DataFrame({\"property_id\": [property_id], \"description\": [description]})\n",
    "                ], ignore_index=True)\n",
    "        elif self.db_type == \"pyspark\":\n",
    "            if not self.df_property_type.filter(self.df_property_type.property_id == property_id).count():\n",
    "                new_row = self.spark.createDataFrame([(property_id, description)], schema=\"property_id STRING, description STRING\")\n",
    "                self.df_property_type = self.df_property_type.union(new_row)\n",
    "        else:\n",
    "            self.cursor.execute(f\"\"\"\n",
    "                INSERT OR IGNORE INTO {self.name}_property_type (property_id, description)\n",
    "                VALUES (?, ?)\n",
    "            \"\"\", (property_id, description))\n",
    "            self.conn.commit()\n",
    "\n",
    "    def add_property(self, object_id, property_id, value, check_property_type=False):\n",
    "        \"\"\"Pridedama savybė konkrečiam objektui.\"\"\"\n",
    "        if self.db_type == \"dataframe\":\n",
    "            if check_property_type and property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.df_property = pd.concat([\n",
    "                self.df_property,\n",
    "                pd.DataFrame({\"object_id\": [object_id], \"property_id\": [property_id], \"value\": [value]})\n",
    "            ], ignore_index=True).drop_duplicates(subset=[\"object_id\", \"property_id\"])\n",
    "        elif self.db_type == \"pyspark\":\n",
    "            if check_property_type and not self.df_property_type.filter(self.df_property_type.property_id == property_id).count():\n",
    "                raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            new_row = self.spark.createDataFrame([(object_id, property_id, value)], schema=\"object_id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property = self.df_property.union(new_row)\n",
    "        else:\n",
    "            if check_property_type:\n",
    "                self.cursor.execute(f\"\"\"\n",
    "                    SELECT 1 FROM {self.name}_property_type WHERE property_id = ?\n",
    "                \"\"\", (property_id,))\n",
    "                if not self.cursor.fetchone():\n",
    "                    raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.cursor.execute(f\"\"\"\n",
    "                INSERT OR REPLACE INTO {self.name}_property (object_id, property_id, value)\n",
    "                VALUES (?, ?, ?)\n",
    "            \"\"\", (object_id, property_id, value))\n",
    "            self.conn.commit()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Uždaromas ryšys su duomenų baze, jei naudojama SQLite.\"\"\"\n",
    "        if self.db_type == \"sqlite\":\n",
    "            self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite Test\n"
     ]
    }
   ],
   "source": [
    "# SQLite testavimas\n",
    "print(\"SQLite Test\")\n",
    "sqlite_test = properties_of(\"testas\", db=\":memory:\")\n",
    "sqlite_test.add_property_type(\"name\", \"Vardas\")\n",
    "sqlite_test.add_property(\"1\", \"name\", \"John\")\n",
    "# print(sqlite_test.df_property)\n",
    "# sqlite_test.df_property.show()\n",
    "sqlite_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pandas Test\n",
      "  object_id property_id value\n",
      "0         1        name  Jane\n"
     ]
    }
   ],
   "source": [
    "# Pandas DF testavimas\n",
    "print(\"\\nPandas Test\")\n",
    "pandas_test = properties_of(\"testas\", db=pd.DataFrame())\n",
    "pandas_test.add_property_type(\"name\", \"Vardas\")\n",
    "pandas_test.add_property(\"1\", \"name\", \"Jane\")\n",
    "print(pandas_test.df_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PySpark Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+\n",
      "|object_id|property_id|   value|\n",
      "+---------+-----------+--------+\n",
      "|        1|       name|    John|\n",
      "|        1|    surname|    Gira|\n",
      "|      222|       name|  Marija|\n",
      "|      222|    surname|Šmitaitė|\n",
      "+---------+-----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# PySpark DF testavimas\n",
    "print(\"\\nPySpark Test\")\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "pyspark_test = properties_of(\"testas\",\n",
    "    db=spark.createDataFrame([],\"object_id STRING, property_id STRING, value STRING\"))\n",
    "pyspark_test.add_property_type(\"name\", \"Vardas\")\n",
    "pyspark_test.add_property(\"1\", \"name\", \"John\")\n",
    "pyspark_test.add_property(\"1\", \"surname\", \"Gira\")\n",
    "pyspark_test.add_property(\"222\", \"name\", \"Marija\")\n",
    "pyspark_test.add_property(\"222\", \"surname\", \"Šmitaitė\")\n",
    "pyspark_test.df_property.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
