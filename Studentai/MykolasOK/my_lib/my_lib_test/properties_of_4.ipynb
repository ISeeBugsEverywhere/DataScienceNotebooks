{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "# from pyspark.sql import SparkSession\n",
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "class properties_of:\n",
    "    def __init__(self, name: str, engine: str = \"sqlite_memory\", db_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Inicijuoja objektą su pasirenkamu duomenų bazės \"varikliu\".\n",
    "\n",
    "        :param name: Objekto pavadinimas (naudojamas lentelių pavadinimams kurti).\n",
    "        :param engine: \n",
    "            - \"sqlite_memory\": SQLite laikoma atmintyje.\n",
    "            - \"sqlite_file\": SQLite laikoma faile, reikia nurodyti `db_path`.\n",
    "            - \"pandas\": Pandas DataFrame kaip duomenų bazė.\n",
    "            - \"pyspark\": PySpark DataFrame kaip duomenų bazė.\n",
    "        :param db_path: Naudojamas tik jei `engine=\"sqlite_file\"`, nurodo SQLite failo kelią.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.engine = engine\n",
    "\n",
    "        if engine == \"sqlite_memory\":\n",
    "            # SQLite laikoma atmintyje\n",
    "            self.conn = sqlite3.connect(\":memory:\")\n",
    "            self.cursor = self.conn.cursor()\n",
    "            self._create_tables()\n",
    "        elif engine == \"sqlite_file\":\n",
    "            # SQLite laikoma faile\n",
    "            if not db_path:\n",
    "                raise ValueError(\"Jei pasirenkama 'sqlite_file', turi būti nurodytas 'db_path'.\")\n",
    "            self.conn = sqlite3.connect(db_path)\n",
    "            self.cursor = self.conn.cursor()\n",
    "            self._create_tables()\n",
    "        elif engine == \"pandas\":\n",
    "            # Pandas DataFrame naudojamas kaip duomenų bazė\n",
    "            self.df_property = pd.DataFrame(columns=[\"object_id\", \"property_id\", \"value\"])\n",
    "            self.df_property_type = pd.DataFrame(columns=[\"property_id\", \"description\"])\n",
    "        elif engine == \"pyspark\":\n",
    "            # PySpark DataFrame naudojamas kaip duomenų bazė\n",
    "            self.spark = SparkSession.builder.master(\"local\").appName(\"PropertiesDB\").getOrCreate()\n",
    "            self.df_property = self.spark.createDataFrame([], schema=\"object_id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property_type = self.spark.createDataFrame([], schema=\"property_id STRING, description STRING\")\n",
    "        else:\n",
    "            raise ValueError(\"Nepalaikomas duomenų bazės variklis: pasirinkite 'sqlite_memory', 'sqlite_file', 'pandas' arba 'pyspark'.\")\n",
    "\n",
    "    def _create_tables(self) -> None:\n",
    "        \"\"\"Sukuriamos lentelės SQLite duomenų bazėje.\"\"\"\n",
    "        self.cursor.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.name}_property (\n",
    "                object_id TEXT,\n",
    "                property_id TEXT,\n",
    "                value TEXT,\n",
    "                PRIMARY KEY (object_id, property_id)\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.cursor.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.name}_property_type (\n",
    "                property_id TEXT PRIMARY KEY,\n",
    "                description TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def add_property_type(self, property_id: str, description: str) -> None:\n",
    "        \"\"\"Pridedamas savybės tipas.\"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            if property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                self.df_property_type = pd.concat([\n",
    "                    self.df_property_type,\n",
    "                    pd.DataFrame({\"property_id\": [property_id], \"description\": [description]})\n",
    "                ], ignore_index=True)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            existing = self.df_property_type.filter(f\"property_id = '{property_id}'\").count() > 0\n",
    "            if not existing:\n",
    "                new_row = self.spark.createDataFrame([(property_id, description)], schema=\"property_id STRING, description STRING\")\n",
    "                self.df_property_type = self.df_property_type.union(new_row)\n",
    "        else:\n",
    "            self.cursor.execute(f\"\"\"\n",
    "                INSERT OR IGNORE INTO {self.name}_property_type (property_id, description)\n",
    "                VALUES (?, ?)\n",
    "            \"\"\", (property_id, description))\n",
    "            self.conn.commit()\n",
    "\n",
    "    def add_property(self, object_id: str, property_id: str, value: str, check_property_type: bool = False) -> None:\n",
    "        \"\"\"Pridedama savybė konkrečiam objektui.\"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            if check_property_type and property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.df_property = pd.concat([\n",
    "                self.df_property,\n",
    "                pd.DataFrame({\"object_id\": [object_id], \"property_id\": [property_id], \"value\": [value]})\n",
    "            ], ignore_index=True).drop_duplicates(subset=[\"object_id\", \"property_id\"])\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if check_property_type:\n",
    "                existing = self.df_property_type.filter(f\"property_id = '{property_id}'\").count() > 0\n",
    "                if not existing:\n",
    "                    raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            new_row = self.spark.createDataFrame([(object_id, property_id, value)], schema=\"object_id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property = self.df_property.union(new_row)\n",
    "        else:\n",
    "            if check_property_type:\n",
    "                self.cursor.execute(f\"\"\"\n",
    "                    SELECT 1 FROM {self.name}_property_type WHERE property_id = ?\n",
    "                \"\"\", (property_id,))\n",
    "                if not self.cursor.fetchone():\n",
    "                    raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.cursor.execute(f\"\"\"\n",
    "                INSERT OR REPLACE INTO {self.name}_property (object_id, property_id, value)\n",
    "                VALUES (?, ?, ?)\n",
    "            \"\"\", (object_id, property_id, value))\n",
    "            self.conn.commit()\n",
    "\n",
    "    def get_all_properties(self) -> Union[pd.DataFrame, None]:\n",
    "        \"\"\"Gaunamos visos savybės visiems objektams kaip Pandas DataFrame.\"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            return self.df_property.merge(self.df_property_type, on=\"property_id\", how=\"left\")\n",
    "        elif self.engine == \"pyspark\":\n",
    "            return self.df_property.join(self.df_property_type, \"property_id\", \"left\").toPandas()\n",
    "        else:\n",
    "            self.cursor.execute(f\"\"\"\n",
    "                SELECT p.object_id, p.property_id, pt.description, p.value\n",
    "                FROM {self.name}_property p\n",
    "                LEFT JOIN {self.name}_property_type pt\n",
    "                ON p.property_id = pt.property_id\n",
    "            \"\"\")\n",
    "            rows = self.cursor.fetchall()\n",
    "            return pd.DataFrame(rows, columns=[\"object_id\", \"property_id\", \"description\", \"value\"])\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Uždaromas SQLite ryšys arba PySpark sesija.\"\"\"\n",
    "        if self.engine.startswith(\"sqlite\"):\n",
    "            self.conn.close()\n",
    "        elif self.engine == \"pyspark\":\n",
    "            self.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     object_id property_id  description       value\n",
      "0  111-222-333       title  Pavadinimas  Lapė Snapė\n",
      "\n",
      "     object_id property_id       value  description\n",
      "0  111-222-333       title  Lapė Snapė  Pavadinimas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    # Demonstracija SQLite atmintyje\n",
    "    obj_memory = properties_of(\"knyga\", engine=\"sqlite_memory\")\n",
    "    obj_memory.add_property_type(\"title\", \"Pavadinimas\")\n",
    "    obj_memory.add_property(\"111-222-333\", \"title\", \"Lapė Snapė\")\n",
    "    print(obj_memory.get_all_properties())\n",
    "    obj_memory.close()\n",
    "    print()\n",
    "\n",
    "    # Pandas DF demonstracija\n",
    "    obj_pandas = properties_of(\"knyga\", engine=\"pandas\")\n",
    "    obj_pandas.add_property_type(\"title\", \"Pavadinimas\")\n",
    "    obj_pandas.add_property(\"111-222-333\", \"title\", \"Lapė Snapė\")\n",
    "    print(obj_pandas.get_all_properties())\n",
    "    print()\n",
    "\n",
    "    # PySpark DF demonstracija\n",
    "    # obj_pyspark = properties_of(\"knyga\", engine=\"pyspark\")\n",
    "    # obj_pyspark.add_property_type(\"title\", \"Pavadinimas\")\n",
    "    # obj_pyspark.add_property(\"111-222-333\", \"title\", \"Lapė Snapė\")\n",
    "    # print(obj_pyspark.get_all_properties())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  object_id property_id  description             value\n",
      "0       aaa       title  Pavadinimas        Lapė Snapė\n",
      "1       bbb       title  Pavadinimas  Mėnuo Juodaragis\n",
      "2       ccc       title  Pavadinimas         Meškiukas\n",
      "\n",
      "     object_id property_id          value      description\n",
      "0  111-222-333       title    Panda Bamba      Pavadinimas\n",
      "1  222-333-111       title  Mėnuo Beragis      Pavadinimas\n",
      "2  333-111-222       title         Kurmis      Pavadinimas\n",
      "3  111-222-333       metai           1900  Išleidimo metai\n",
      "4  222-333-111       metai           1999  Išleidimo metai\n",
      "5  333-111-222       metai           2020  Išleidimo metai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Demonstracija SQLite atmintyje\n",
    "    obj_memory = properties_of(\"knyga\", engine=\"sqlite_memory\")\n",
    "    obj_memory.add_property_type(\"title\", \"Pavadinimas\")\n",
    "    obj_memory.add_property_type(\"metai\", \"Išleidimo metai\")\n",
    "    obj_memory.add_property(\"aaa\", \"title\", \"Lapė Snapė\")\n",
    "    obj_memory.add_property(\"bbb\", \"title\", \"Mėnuo Juodaragis\")\n",
    "    obj_memory.add_property(\"ccc\", \"title\", \"Meškiukas\")\n",
    "    print(obj_memory.get_all_properties())\n",
    "    obj_memory.close()\n",
    "    print()\n",
    "\n",
    "    # Pandas DF demonstracija\n",
    "    obj_pandas = properties_of(\"knyga\", engine=\"pandas\")\n",
    "    obj_pandas.add_property_type(\"title\", \"Pavadinimas\")\n",
    "    obj_pandas.add_property_type(\"metai\", \"Išleidimo metai\")\n",
    "    obj_pandas.add_property(\"111-222-333\", \"title\", \"Panda Bamba\")\n",
    "    obj_pandas.add_property(\"222-333-111\", \"title\", \"Mėnuo Beragis\")\n",
    "    obj_pandas.add_property(\"333-111-222\", \"title\", \"Kurmis\")\n",
    "    obj_pandas.add_property(\"111-222-333\", \"metai\", \"1900\")\n",
    "    obj_pandas.add_property(\"222-333-111\", \"metai\", \"1999\")\n",
    "    obj_pandas.add_property(\"333-111-222\", \"metai\", \"2020\")\n",
    "    print(obj_pandas.get_all_properties())\n",
    "    print()\n",
    "\n",
    "    # PySpark DF demonstracija\n",
    "    # obj_pyspark = properties_of(\"knyga\", engine=\"pyspark\")\n",
    "    # obj_pyspark.add_property_type(\"title\", \"Pavadinimas\")\n",
    "    # obj_pyspark.add_property(\"111-222-333\", \"title\", \"Lapė Snapė\")\n",
    "    # obj_pyspark.df_property.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
