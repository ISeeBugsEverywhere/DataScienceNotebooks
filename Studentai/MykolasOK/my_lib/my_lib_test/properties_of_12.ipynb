{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import List, Union\n",
    "\n",
    "class properties_of:\n",
    "    def __init__(self, name: str, engine: str = \"pandas\"):\n",
    "        \"\"\"\n",
    "        Inicializuojama klasė. Palaikomi du varikliai: 'pandas' ir 'pyspark'.\n",
    "\n",
    "        :param name: Objektų grupės pavadinimas.\n",
    "        :param engine: Variklis ('pandas' arba 'pyspark').\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.engine = engine\n",
    "\n",
    "        if engine == \"pandas\":\n",
    "            import pandas as pd\n",
    "            self.pd = pd\n",
    "            self.df_property = pd.DataFrame(columns=[\"id\", \"property_id\", \"value\"])\n",
    "            self.df_property_type = pd.DataFrame(columns=[\"property_id\", \"description\"])\n",
    "        elif engine == \"pyspark\":\n",
    "            from pyspark.sql import SparkSession\n",
    "            self.spark = SparkSession.builder.master(\"local\").appName(\"PropertiesDB\").getOrCreate()\n",
    "            self.df_property = self.spark.createDataFrame([], schema=\"id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property_type = self.spark.createDataFrame([], schema=\"property_id STRING, description STRING\")\n",
    "        else:\n",
    "            raise ValueError(\"Nepalaikomas variklis: pasirinkite 'pandas' arba 'pyspark'.\")\n",
    "\n",
    "    def add_property_type(self, property_id: str, description: str) -> None:\n",
    "        \"\"\"\n",
    "        Pridedamas naujas savybės tipas į tipų lentelę.\n",
    "\n",
    "        :param property_id: Savybės ID.\n",
    "        :param description: Savybės aprašymas.\n",
    "        \"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            self.df_property_type = self.pd.concat([\n",
    "                self.df_property_type,\n",
    "                self.pd.DataFrame({\"property_id\": [property_id], \"description\": [description]})\n",
    "            ], ignore_index=True).drop_duplicates(subset=[\"property_id\"])\n",
    "        elif self.engine == \"pyspark\":\n",
    "            new_row = self.spark.createDataFrame([(property_id, description)], schema=\"property_id STRING, description STRING\")\n",
    "            self.df_property_type = self.df_property_type.union(new_row)\n",
    "\n",
    "    def add_property(self, id: str, property_id: str, value: str, check_property_type: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Pridedama savybė konkrečiam objektui.\n",
    "\n",
    "        :param id: Objekto ID.\n",
    "        :param property_id: Savybės ID.\n",
    "        :param value: Savybės reikšmė.\n",
    "        :param check_property_type: Tikrinti, ar savybės tipas egzistuoja.\n",
    "        \"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            if check_property_type and property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.df_property = self.pd.concat([\n",
    "                self.df_property,\n",
    "                self.pd.DataFrame({\"id\": [id], \"property_id\": [property_id], \"value\": [value]})\n",
    "            ], ignore_index=True).drop_duplicates(subset=[\"id\", \"property_id\"])\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if check_property_type:\n",
    "                existing = self.df_property_type.filter(f\"property_id = '{property_id}'\").count() > 0\n",
    "                if not existing:\n",
    "                    raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            new_row = self.spark.createDataFrame([(id, property_id, value)], schema=\"id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property = self.df_property.union(new_row)\n",
    "\n",
    "    def import_from_narrow(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Importuoja duomenis iš siauros lentelės.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                new_data = self.pd.read_csv(filepath)\n",
    "            elif file_format == \"parquet\":\n",
    "                new_data = self.pd.read_parquet(filepath)\n",
    "            elif file_format == \"feather\":\n",
    "                new_data = self.pd.read_feather(filepath)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "            self.df_property = self.pd.concat([self.df_property, new_data], ignore_index=True).drop_duplicates()\n",
    "            rows = len(new_data)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                new_data = self.spark.read.format(file_format).load(filepath)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas su PySpark.\")\n",
    "            self.df_property = self.df_property.union(new_data)\n",
    "            rows = new_data.count()\n",
    "        print(f\"Importuota {rows} eilučių iš {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def export_to_narrow(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Eksportuoja duomenis į siaurą lentelę.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                self.df_property.to_csv(filepath, index=False)\n",
    "            elif file_format == \"parquet\":\n",
    "                self.df_property.to_parquet(filepath, index=False)\n",
    "            elif file_format == \"feather\":\n",
    "                self.df_property.to_feather(filepath)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "            rows = len(self.df_property)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                self.df_property.write.mode(\"overwrite\").format(file_format).save(filepath)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas su PySpark.\")\n",
    "            rows = self.df_property.count()\n",
    "        print(f\"Eksportuota {rows} eilučių į {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def import_from_file(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Importuoja duomenis iš failo.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather', 'sqlite'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                self.df_property = self.pd.read_csv(filepath)\n",
    "            elif file_format == \"parquet\":\n",
    "                self.df_property = self.pd.read_parquet(filepath)\n",
    "            elif file_format == \"feather\":\n",
    "                self.df_property = self.pd.read_feather(filepath)\n",
    "            elif file_format == \"sqlite\":\n",
    "                import sqlite3\n",
    "                with sqlite3.connect(filepath) as conn:\n",
    "                    self.df_property = self.pd.read_sql(f\"SELECT * FROM {self.name}_property\", conn)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "            rows = len(self.df_property)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                self.df_property = self.spark.read.format(file_format).load(filepath)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas su PySpark.\")\n",
    "            rows = self.df_property.count()\n",
    "        print(f\"Importuota {rows} eilučių iš {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def export_to_file(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Eksportuoja duomenis į failą.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather', 'sqlite'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                self.df_property.to_csv(filepath, index=False)\n",
    "            elif file_format == \"parquet\":\n",
    "                self.df_property.to_parquet(filepath, index=False)\n",
    "            elif file_format == \"feather\":\n",
    "                self.df_property.to_feather(filepath)\n",
    "            elif file_format == \"sqlite\":\n",
    "                import sqlite3\n",
    "                with sqlite3.connect(filepath) as conn:\n",
    "                    self.df_property.to_sql(f\"{self.name}_property\", conn, if_exists=\"replace\", index=False)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "            rows = len(self.df_property)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                self.df_property.write.mode(\"overwrite\").format(file_format).save(filepath)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas su PySpark.\")\n",
    "            rows = self.df_property.count()\n",
    "        print(f\"Eksportuota {rows} eilučių į {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Uždaroma PySpark sesija, jei naudojama.\"\"\"\n",
    "        if self.engine == \"pyspark\":\n",
    "            self.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukurtas 1000 objektų sąrašas su atsitiktinėmis savybėmis.\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    import string\n",
    "\n",
    "    # NATO fonetinės abėcėlės sąrašas\n",
    "    nato_phonetic_alphabet = [\n",
    "        \"Alpha\", \"Bravo\", \"Charlie\", \"Delta\", \"Echo\", \"Foxtrot\",\n",
    "        \"Golf\", \"Hotel\", \"India\", \"Juliett\", \"Kilo\", \"Lima\",\n",
    "        \"Mike\", \"November\", \"Oscar\", \"Papa\", \"Quebec\", \"Romeo\",\n",
    "        \"Sierra\", \"Tango\", \"Uniform\", \"Victor\", \"Whiskey\", \"X-ray\", \"Yankee\", \"Zulu\"\n",
    "    ]\n",
    "\n",
    "    # Sukuriamas objektas\n",
    "    obj = properties_of(\"random_objects\", engine=\"pandas\")\n",
    "\n",
    "    # Įrašomos savybės\n",
    "    for phonetic in nato_phonetic_alphabet:\n",
    "        obj.add_property_type(property_id=phonetic.lower(), description=f\"Savybė {phonetic}\")\n",
    "\n",
    "    # Generuojamas milijonas objektų su atsitiktinėmis savybėmis\n",
    "    num_objects = 1_000\n",
    "    for i in range(1, num_objects + 1):\n",
    "        object_id = f\"obj_{i}\"\n",
    "        for _ in range(3):  # Trijų savybių priskyrimas\n",
    "            property_id = random.choice(nato_phonetic_alphabet).lower()\n",
    "            value = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n",
    "            obj.add_property(object_id, property_id, value)\n",
    "\n",
    "    print(f\"Sukurtas {num_objects} objektų sąrašas su atsitiktinėmis savybėmis.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
