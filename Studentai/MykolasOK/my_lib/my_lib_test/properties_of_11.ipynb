{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "\n",
    "class properties_of:\n",
    "    def __init__(self, name: str, engine: str = \"pandas\"):\n",
    "        \"\"\"\n",
    "        Inicializuojama klasė. Palaikomi du varikliai: 'pandas' ir 'pyspark'.\n",
    "\n",
    "        :param name: Objektų grupės pavadinimas.\n",
    "        :param engine: Variklis ('pandas' arba 'pyspark').\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.engine = engine\n",
    "\n",
    "        if engine == \"pandas\":\n",
    "            import pandas as pd\n",
    "            self.pd = pd\n",
    "            self.df_property = pd.DataFrame(columns=[\"id\", \"property_id\", \"value\"])\n",
    "            self.df_property_type = pd.DataFrame(columns=[\"property_id\", \"description\"])\n",
    "        elif engine == \"pyspark\":\n",
    "            from pyspark.sql import SparkSession\n",
    "            self.spark = SparkSession.builder.master(\"local\").appName(\"PropertiesDB\").getOrCreate()\n",
    "            self.df_property = self.spark.createDataFrame([], schema=\"id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property_type = self.spark.createDataFrame([], schema=\"property_id STRING, description STRING\")\n",
    "        else:\n",
    "            raise ValueError(\"Nepalaikomas variklis: pasirinkite 'pandas' arba 'pyspark'.\")\n",
    "\n",
    "    def add_property(self, id: str, property_id: str, value: str, check_property_type: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Pridedama savybė konkrečiam objektui.\n",
    "\n",
    "        :param id: Objekto ID.\n",
    "        :param property_id: Savybės ID.\n",
    "        :param value: Savybės reikšmė.\n",
    "        :param check_property_type: Tikrinti, ar savybės tipas egzistuoja.\n",
    "        \"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            if check_property_type and property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.df_property = self.pd.concat([\n",
    "                self.df_property,\n",
    "                self.pd.DataFrame({\"id\": [id], \"property_id\": [property_id], \"value\": [value]})\n",
    "            ], ignore_index=True).drop_duplicates(subset=[\"id\", \"property_id\"])\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if check_property_type:\n",
    "                existing = self.df_property_type.filter(f\"property_id = '{property_id}'\").count() > 0\n",
    "                if not existing:\n",
    "                    raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            new_row = self.spark.createDataFrame([(id, property_id, value)], schema=\"id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property = self.df_property.union(new_row)\n",
    "\n",
    "    def export_to_file(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Eksportuoja duomenis į failą.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather', 'sqlite'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                self.df_property.to_csv(filepath, index=False)\n",
    "            elif file_format == \"parquet\":\n",
    "                self.df_property.to_parquet(filepath, index=False)\n",
    "            elif file_format == \"feather\":\n",
    "                self.df_property.to_feather(filepath)\n",
    "            elif file_format == \"sqlite\":\n",
    "                import sqlite3\n",
    "                with sqlite3.connect(filepath) as conn:\n",
    "                    self.df_property.to_sql(f\"{self.name}_property\", conn, if_exists=\"replace\", index=False)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "            rows = len(self.df_property)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                self.df_property.write.mode(\"overwrite\").format(file_format).save(filepath)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas su PySpark.\")\n",
    "            rows = self.df_property.count()\n",
    "        print(f\"Eksportuota {rows} eilučių į {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def import_from_file(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Importuoja duomenis iš failo.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather', 'sqlite'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                self.df_property = self.pd.read_csv(filepath)\n",
    "            elif file_format == \"parquet\":\n",
    "                self.df_property = self.pd.read_parquet(filepath)\n",
    "            elif file_format == \"feather\":\n",
    "                self.df_property = self.pd.read_feather(filepath)\n",
    "            elif file_format == \"sqlite\":\n",
    "                import sqlite3\n",
    "                with sqlite3.connect(filepath) as conn:\n",
    "                    self.df_property = self.pd.read_sql(f\"SELECT * FROM {self.name}_property\", conn)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "            rows = len(self.df_property)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                self.df_property = self.spark.read.format(file_format).load(filepath)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas su PySpark.\")\n",
    "            rows = self.df_property.count()\n",
    "        print(f\"Importuota {rows} eilučių iš {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Uždaromas PySpark sesija, jei naudojama.\"\"\"\n",
    "        if self.engine == \"pyspark\":\n",
    "            self.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testavimas su Pandas\n",
      "Eksportuota 1 eilučių į pandas_test.csv. Trukmė: 0.00s\n",
      "Importuota 1 eilučių iš pandas_test.csv. Trukmė: 0.00s\n",
      "Pandas lentelė:\n",
      "            id property_id       value\n",
      "0  111-222-333       title  Lapė Snapė\n",
      "\n",
      "Testavimas su PySpark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eksportuota 1 eilučių į spark_test.parquet. Trukmė: 2.54s\n",
      "Importuota 1 eilučių iš spark_test.parquet. Trukmė: 0.50s\n",
      "PySpark lentelė:\n",
      "+------+-----------+------+\n",
      "|    id|property_id| value|\n",
      "+------+-----------+------+\n",
      "|GGZ123|       fuel|diesel|\n",
      "+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    print('Testavimas su Pandas')\n",
    "    obj_pandas = properties_of(\"knyga\", engine=\"pandas\")\n",
    "    obj_pandas.add_property(\"111-222-333\", \"title\", \"Lapė Snapė\")\n",
    "    obj_pandas.export_to_file(\"pandas_test.csv\", \"csv\")\n",
    "    obj_pandas.import_from_file(\"pandas_test.csv\", \"csv\")\n",
    "    print(\"Pandas lentelė:\")\n",
    "    print(obj_pandas.df_property)\n",
    "    obj_pandas.close()\n",
    "\n",
    "    print()\n",
    "\n",
    "    print('Testavimas su PySpark')\n",
    "    obj_spark = properties_of(\"automobilis\", engine=\"pyspark\")\n",
    "    obj_spark.add_property(\"GGZ123\", \"fuel\", \"diesel\")\n",
    "    obj_spark.export_to_file(\"spark_test.parquet\", \"parquet\")\n",
    "    obj_spark.import_from_file(\"spark_test.parquet\", \"parquet\")\n",
    "    print(\"PySpark lentelė:\")\n",
    "    obj_spark.df_property.show()\n",
    "    obj_spark.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
