{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class properties_of:\n",
    "    \"\"\"\n",
    "    Klasė darbui su objektų savybėmis įvairiuose duomenų varikliuose.\n",
    "\n",
    "    Parametrai:\n",
    "        - name (str): Objekto grupės pavadinimas.\n",
    "        - engine (str): Naudojamas duomenų variklis (\"pandas\" arba \"pyspark\").\n",
    "        - silent (bool): Jei True, nenaudojami išvesties pranešimai.\n",
    "\n",
    "    Savybės:\n",
    "        - property_type (DataFrame): Laikomi savybių tipai.\n",
    "        - property (DataFrame): Laikomos objektų savybės.\n",
    "        - file_path (str): Paskutiniu metu skaityto ar rašyto failo kelias.\n",
    "        - file_format (str): Failo formatas (csv, parquet, feather).\n",
    "        - opened_format (str): Atidarytas lentelės formatas (\"wide\" arba \"narrow\").\n",
    "\n",
    "    Vieši metodai:\n",
    "        - add_property_type(property_id: str, description: str) -> None\n",
    "            Prideda naują savybių tipą.\n",
    "        - add_property(object_id: str, property_id: str, value: Any) -> None\n",
    "            Prideda arba atnaujina savybę konkrečiam objektui.\n",
    "        - open_wide(file_path: str, file_format: str) -> None\n",
    "            Perskaito plačią lentelę iš failo ir išsaugo failo informaciją.\n",
    "        - open_narrow(file_path: str, file_format: str) -> None\n",
    "            Perskaito siaurą lentelę iš failo ir išsaugo failo informaciją.\n",
    "        - write_wide(file_path: str, file_format: str, columns: List[str] = []) -> None\n",
    "            Išsaugo plačią lentelę į failą.\n",
    "        - write_narrow(file_path: str, file_format: str, columns: List[str] = []) -> None\n",
    "            Išsaugo siaurą lentelę į failą.\n",
    "        - get_wide_df(columns: List[str] = []) -> DataFrame\n",
    "            Grąžina plačią lentelę. Jei `columns` yra tuščias, grąžina visus stulpelius.\n",
    "        - save() -> None\n",
    "            Išsaugo duomenis į anksčiau perskaitytą failą (wide arba narrow).\n",
    "        - close() -> None\n",
    "            Uždaro PySpark sesiją ir atlaisvina atmintį (RAM).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, engine: str = \"pandas\", silent: bool = False) -> None:\n",
    "        from typing import List\n",
    "        import time\n",
    "\n",
    "        self.name = name\n",
    "        self.engine = engine\n",
    "        self.silent = silent\n",
    "        self.file_path = None\n",
    "        self.file_format = None\n",
    "        self.opened_format = None\n",
    "\n",
    "        if engine == \"pandas\":\n",
    "            import pandas as pd\n",
    "            self.pd = pd\n",
    "            self.property_type = pd.DataFrame(columns=[\"property_id\", \"description\"])\n",
    "            self.property = pd.DataFrame(columns=[\"id\", \"property_id\", \"value\"])\n",
    "        elif engine == \"pyspark\":\n",
    "            from pyspark.sql import SparkSession\n",
    "            self.spark = SparkSession.builder.appName(\"Properties\").getOrCreate()\n",
    "            self.property_type = self.spark.createDataFrame([], \"property_id STRING, description STRING\")\n",
    "            self.property = self.spark.createDataFrame([], \"id STRING, property_id STRING, value STRING\")\n",
    "        else:\n",
    "            raise ValueError(f\"Nežinomas variklis: {engine}\")\n",
    "\n",
    "    def _log(self, message: str) -> None:\n",
    "        \"\"\"Išveda pranešimą, jei silent=False.\"\"\"\n",
    "        if not self.silent:\n",
    "            print(message)\n",
    "\n",
    "    def add_property_type(self, property_id: str, description: str) -> None:\n",
    "        \"\"\"Prideda naują savybių tipą.\"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            new_row = self.pd.DataFrame({\"property_id\": [property_id], \"description\": [description]})\n",
    "            self.property_type = self.pd.concat([self.property_type, new_row], ignore_index=True).drop_duplicates()\n",
    "        elif self.engine == \"pyspark\":\n",
    "            new_row = self.spark.createDataFrame([(property_id, description)], schema=\"property_id STRING, description STRING\")\n",
    "            self.property_type = self.property_type.union(new_row)\n",
    "\n",
    "    def add_property(self, id: str, property_id: str, value: str) -> None:\n",
    "        \"\"\"Prideda arba atnaujina savybę konkrečiam objektui.\"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            condition = (self.property[\"id\"] == id) & (self.property[\"property_id\"] == property_id)\n",
    "            if condition.any():\n",
    "                self.property.loc[condition, \"value\"] = value\n",
    "            else:\n",
    "                new_row = self.pd.DataFrame({\"id\": [id], \"property_id\": [property_id], \"value\": [value]})\n",
    "                self.property = self.pd.concat([self.property, new_row], ignore_index=True).drop_duplicates()\n",
    "        elif self.engine == \"pyspark\":\n",
    "            self.property = (\n",
    "                self.property.filter(f\"id != '{id}' OR property_id != '{property_id}'\")\n",
    "                .union(self.spark.createDataFrame([(id, property_id, value)], schema=\"id STRING, property_id STRING, value STRING\"))\n",
    "            )\n",
    "\n",
    "    def open_narrow(self, file_path: str, file_format: str) -> None:\n",
    "        \"\"\"Perskaito siaurą lentelę iš failo.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.file_path = file_path\n",
    "        self.file_format = file_format\n",
    "        self.opened_format = \"narrow\"\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                self.property = self.pd.read_csv(file_path)\n",
    "            elif file_format == \"parquet\":\n",
    "                self.property = self.pd.read_parquet(file_path)\n",
    "            elif file_format == \"feather\":\n",
    "                self.property = self.pd.read_feather(file_path)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                self.property = self.spark.read.format(file_format).load(file_path)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "        self._log(f\"Siauros lentelės duomenys perskaityti per {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "    def open_wide(self, file_path: str, file_format: str) -> None:\n",
    "        \"\"\"Perskaito plačią lentelę iš failo ir konvertuoja ją į siaurą.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.file_path = file_path\n",
    "        self.file_format = file_format\n",
    "        self.opened_format = \"wide\"\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                wide_df = self.pd.read_csv(file_path)\n",
    "            elif file_format == \"parquet\":\n",
    "                wide_df = self.pd.read_parquet(file_path)\n",
    "            elif file_format == \"feather\":\n",
    "                wide_df = self.pd.read_feather(file_path)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "            narrow_df = wide_df.melt(id_vars=\"id\", var_name=\"property_id\", value_name=\"value\")\n",
    "            self.property = self.pd.concat([self.property, narrow_df], ignore_index=True).drop_duplicates()\n",
    "        self._log(f\"Plačios lentelės duomenys perskaityti per {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "    def write_narrow(self, file_path: str, file_format: str, columns: List[str] = []) -> None:\n",
    "        \"\"\"Išsaugo siaurą lentelę į failą.\"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            narrow_df = self.property\n",
    "            if columns:\n",
    "                narrow_df = narrow_df[narrow_df[\"property_id\"].isin(columns)]\n",
    "            if file_format == \"csv\":\n",
    "                narrow_df.to_csv(file_path, index=False)\n",
    "            elif file_format == \"parquet\":\n",
    "                narrow_df.to_parquet(file_path, index=False)\n",
    "            elif file_format == \"feather\":\n",
    "                narrow_df.to_feather(file_path)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                narrow_df = self.property\n",
    "                narrow_df.write.mode(\"overwrite\").format(file_format).save(file_path)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "        self._log(f\"Siauros lentelės duomenys išsaugoti į {file_path} per {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "    def write_wide(self, file_path: str, file_format: str, columns: List[str] = []) -> None:\n",
    "        \"\"\"Išsaugo plačią lentelę į failą.\"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            wide_df = self.property.pivot(index=\"id\", columns=\"property_id\", values=\"value\").reset_index()\n",
    "            if columns:\n",
    "                wide_df = wide_df[[\"id\"] + columns]\n",
    "            if file_format == \"csv\":\n",
    "                wide_df.to_csv(file_path, index=False)\n",
    "            elif file_format == \"parquet\":\n",
    "                wide_df.to_parquet(file_path, index=False)\n",
    "            elif file_format == \"feather\":\n",
    "                wide_df.to_feather(file_path)\n",
    "            else:\n",
    "                raise ValueError(\"Nepalaikomas failo formatas.\")\n",
    "        self._log(f\"Plačios lentelės duomenys išsaugoti į {file_path} per {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "    def get_wide_df(self, columns: List[str] = []) -> \"pd.DataFrame\":\n",
    "        \"\"\"Grąžina plačią lentelę.\"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            wide_df = self.property.pivot(index=\"id\", columns=\"property_id\", values=\"value\").reset_index()\n",
    "            if columns:\n",
    "                return wide_df[[\"id\"] + columns]\n",
    "            return wide_df\n",
    "        else:\n",
    "            raise NotImplementedError(\"get_wide_df palaikomas tik Pandas varikliui.\")\n",
    "\n",
    "    def save(self) -> None:\n",
    "        \"\"\"Išsaugo duomenis į anksčiau perskaitytą failą (wide arba narrow).\"\"\"\n",
    "        if not self.file_path or not self.file_format or not self.opened_format:\n",
    "            raise ValueError(\"Failo kelias, formatas ir formatas (wide/narrow) turi būti nustatyti per open_* metodus.\")\n",
    "        if self.opened_format == \"narrow\":\n",
    "            self.write_narrow(self.file_path, self.file_format)\n",
    "        elif self.opened_format == \"wide\":\n",
    "            self.write_wide(self.file_path, self.file_format)\n",
    "        else:\n",
    "            raise ValueError(\"Nepalaikomas išsaugojimo formatas.\")\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Uždaro PySpark sesiją ir sunaikina duomenų rėmelius.\"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            self.property = None\n",
    "            self.property_type = None\n",
    "        elif self.engine == \"pyspark\":\n",
    "            self.spark.stop()\n",
    "            self.property = None\n",
    "            self.property_type = None\n",
    "        self._log(\"Klasės resursai sėkmingai atlaisvinti.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testavimas su Pandas ===\n",
      "Kuriamas Pandas objektas...\n",
      "Pridedami savybių tipai...\n",
      "Generuojami 2500 atsitiktiniai objektai su savybėmis...\n",
      "Atlikta per 34.87 s.\n",
      "\n",
      "* Testuojamas formatas: csv\n",
      "Eksportuojami duomenys į siaurą lentelę...\n",
      "Siauros lentelės duomenys išsaugoti į test_data/pandas_narrow.csv per 0.02s.\n",
      "Eksportuojami duomenys į plačią lentelę...\n",
      "Plačios lentelės duomenys išsaugoti į test_data/pandas_wide.csv per 0.03s.\n",
      "Importuojami duomenys iš siauros lentelės...\n",
      "Siauros lentelės duomenys perskaityti per 0.01s.\n",
      "Importuojami duomenys iš plačios lentelės...\n",
      "Plačios lentelės duomenys perskaityti per 0.06s.\n",
      "Išsaugomi duomenys tuo pačiu formatu...\n",
      "Plačios lentelės duomenys išsaugoti į test_data/pandas_wide.csv per 0.06s.\n",
      "\n",
      "* Testuojamas formatas: parquet\n",
      "Eksportuojami duomenys į siaurą lentelę...\n",
      "Siauros lentelės duomenys išsaugoti į test_data/pandas_narrow.parquet per 0.07s.\n",
      "Eksportuojami duomenys į plačią lentelę...\n",
      "Plačios lentelės duomenys išsaugoti į test_data/pandas_wide.parquet per 0.06s.\n",
      "Importuojami duomenys iš siauros lentelės...\n",
      "Siauros lentelės duomenys perskaityti per 0.06s.\n",
      "Importuojami duomenys iš plačios lentelės...\n",
      "Plačios lentelės duomenys perskaityti per 0.07s.\n",
      "Išsaugomi duomenys tuo pačiu formatu...\n",
      "Plačios lentelės duomenys išsaugoti į test_data/pandas_wide.parquet per 0.07s.\n",
      "\n",
      "* Testuojamas formatas: feather\n",
      "Eksportuojami duomenys į siaurą lentelę...\n",
      "Siauros lentelės duomenys išsaugoti į test_data/pandas_narrow.feather per 0.03s.\n",
      "Eksportuojami duomenys į plačią lentelę...\n",
      "Plačios lentelės duomenys išsaugoti į test_data/pandas_wide.feather per 0.04s.\n",
      "Importuojami duomenys iš siauros lentelės...\n",
      "Siauros lentelės duomenys perskaityti per 0.01s.\n",
      "Importuojami duomenys iš plačios lentelės...\n",
      "Plačios lentelės duomenys perskaityti per 0.06s.\n",
      "Išsaugomi duomenys tuo pačiu formatu...\n",
      "Plačios lentelės duomenys išsaugoti į test_data/pandas_wide.feather per 0.05s.\n",
      "Klasės resursai sėkmingai atlaisvinti.\n",
      "Pandas testavimas baigtas sėkmingai.\n",
      "\n",
      "Testavimas baigtas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    import os\n",
    "    import string\n",
    "    import random\n",
    "\n",
    "    # Testavimo nustatymai\n",
    "    pandas_num_objects = 2500\n",
    "    pyspark_num_objects = 100\n",
    "    file_formats = [\"csv\", \"parquet\", \"feather\"]\n",
    "    test_dir = \"test_data\"\n",
    "\n",
    "    # Sukuriamas testavimo katalogas\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # NATO fonetinės abėcėlės sąrašas\n",
    "    nato_phonetic_alphabet = [\n",
    "        \"Alpha\", \"Bravo\", \"Charlie\", \"Delta\", \"Echo\", \"Foxtrot\", \"Golf\", \n",
    "        \"Hotel\", \"India\", \"Juliett\", \"Kilo\", \"Lima\", \"Mike\", \"November\", \n",
    "        \"Oscar\", \"Papa\", \"Quebec\", \"Romeo\", \"Sierra\", \"Tango\", \n",
    "        \"Uniform\", \"Victor\", \"Whiskey\", \"X-ray\", \"Yankee\", \"Zulu\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(\"=== Testavimas su Pandas ===\")\n",
    "    try:\n",
    "        print(\"Kuriamas Pandas objektas...\")\n",
    "        pandas_obj = properties_of(\"pandas_test\", engine=\"pandas\", silent=False)\n",
    "\n",
    "        print(\"Pridedami savybių tipai...\")\n",
    "        for phonetic in nato_phonetic_alphabet:\n",
    "            pandas_obj.add_property_type(property_id=phonetic.lower(), description=f\"Savybė {phonetic}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(f\"Generuojami {pandas_num_objects} atsitiktiniai objektai su savybėmis...\")\n",
    "        for i in range(1, pandas_num_objects + 1):\n",
    "            object_id = f\"obj_{i}\"\n",
    "            for _ in range(3):  # Trijų atsitiktinių savybių priskyrimas\n",
    "                property_id = random.choice(nato_phonetic_alphabet).lower()\n",
    "                value = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n",
    "                pandas_obj.add_property(object_id, property_id, value)\n",
    "        print(f\"Atlikta per {time.time()-start_time:.2f} s.\")\n",
    "\n",
    "        for fmt in file_formats:\n",
    "            print(f\"\\n* Testuojamas formatas: {fmt}\")\n",
    "\n",
    "            wide_file = os.path.join(test_dir, f\"pandas_wide.{fmt}\")\n",
    "            narrow_file = os.path.join(test_dir, f\"pandas_narrow.{fmt}\")\n",
    "\n",
    "            print(\"Eksportuojami duomenys į siaurą lentelę...\")\n",
    "            pandas_obj.write_narrow(narrow_file, fmt)\n",
    "\n",
    "            print(\"Eksportuojami duomenys į plačią lentelę...\")\n",
    "            pandas_obj.write_wide(wide_file, fmt)\n",
    "\n",
    "            print(\"Importuojami duomenys iš siauros lentelės...\")\n",
    "            pandas_obj.open_narrow(narrow_file, fmt)\n",
    "\n",
    "            print(\"Importuojami duomenys iš plačios lentelės...\")\n",
    "            pandas_obj.open_wide(wide_file, fmt)\n",
    "\n",
    "            print(\"Išsaugomi duomenys tuo pačiu formatu...\")\n",
    "            pandas_obj.save()\n",
    "\n",
    "        pandas_obj.close()\n",
    "        print(\"Pandas testavimas baigtas sėkmingai.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Pandas testavimo klaida: {e}\\n\")\n",
    "\n",
    "    # print(\"=== Testavimas su PySpark ===\")\n",
    "    # try:\n",
    "    #     print(\"Kuriamas PySpark objektas...\")\n",
    "    #     pyspark_obj = properties_of(\"pyspark_test\", engine=\"pyspark\", silent=False)\n",
    "\n",
    "    #     print(\"Pridedami savybių tipai...\")\n",
    "    #     for phonetic in nato_phonetic_alphabet:\n",
    "    #         pyspark_obj.add_property_type(property_id=phonetic.lower(), description=f\"Savybė {phonetic}\")\n",
    "\n",
    "    #     print(f\"Generuojami {pyspark_num_objects} atsitiktiniai objektai su savybėmis...\")\n",
    "    #     for i in range(1, pyspark_num_objects + 1):\n",
    "    #         object_id = f\"obj_{i}\"\n",
    "    #         for _ in range(3):  # Trijų atsitiktinių savybių priskyrimas\n",
    "    #             property_id = random.choice(nato_phonetic_alphabet).lower()\n",
    "    #             value = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n",
    "    #             pyspark_obj.add_property(object_id, property_id, value)\n",
    "\n",
    "    #     for fmt in file_formats[:-1]:  # Feather nepalaikomas PySpark\n",
    "    #         print(f\"Testuojamas formatas: {fmt}\")\n",
    "\n",
    "    #         narrow_file = os.path.join(test_dir, f\"pyspark_narrow.{fmt}\")\n",
    "\n",
    "    #         print(\"Eksportuojami duomenys į siaurą lentelę...\")\n",
    "    #         pyspark_obj.write_narrow(narrow_file, fmt)\n",
    "\n",
    "    #         print(\"Importuojami duomenys iš siauros lentelės...\")\n",
    "    #         pyspark_obj.open_narrow(narrow_file, fmt)\n",
    "\n",
    "    #         print(\"Išsaugomi duomenys tuo pačiu formatu...\")\n",
    "    #         pyspark_obj.save()\n",
    "\n",
    "    #     pyspark_obj.close()\n",
    "    #     print(\"PySpark testavimas baigtas sėkmingai.\\n\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"PySpark testavimo klaida: {e}\\n\")\n",
    "\n",
    "    # Valymas\n",
    "    # print(\"Valomas testavimo katalogas...\")\n",
    "    # for file in os.listdir(test_dir):\n",
    "    #     os.remove(os.path.join(test_dir, file))\n",
    "    # os.rmdir(test_dir)\n",
    "\n",
    "    print(\"Testavimas baigtas.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
