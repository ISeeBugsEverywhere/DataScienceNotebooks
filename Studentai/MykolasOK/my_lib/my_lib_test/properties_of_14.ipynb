{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import List, Union\n",
    "\n",
    "class properties_of:\n",
    "    def __init__(self, name: str, engine: str = \"pandas\"):\n",
    "        \"\"\"\n",
    "        Inicializuojama klasė. Palaikomi du varikliai: 'pandas' ir 'pyspark'.\n",
    "\n",
    "        :param name: Objektų grupės pavadinimas.\n",
    "        :param engine: Variklis ('pandas' arba 'pyspark').\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.engine = engine\n",
    "\n",
    "        if engine == \"pandas\":\n",
    "            import pandas as pd\n",
    "            self.pd = pd\n",
    "            self.df_property = pd.DataFrame(columns=[\"id\", \"property_id\", \"value\"])\n",
    "            self.df_property_type = pd.DataFrame(columns=[\"property_id\", \"description\"])\n",
    "        elif engine == \"pyspark\":\n",
    "            from pyspark.sql import SparkSession\n",
    "            self.spark = SparkSession.builder.master(\"local\").appName(\"PropertiesDB\").getOrCreate()\n",
    "            self.df_property = self.spark.createDataFrame([], schema=\"id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property_type = self.spark.createDataFrame([], schema=\"property_id STRING, description STRING\")\n",
    "        else:\n",
    "            raise ValueError(\"Nepalaikomas variklis: pasirinkite 'pandas' arba 'pyspark'.\")\n",
    "\n",
    "    def add_property_type(self, property_id: str, description: str) -> None:\n",
    "        \"\"\"\n",
    "        Pridedamas naujas savybės tipas į tipų lentelę.\n",
    "\n",
    "        :param property_id: Savybės ID.\n",
    "        :param description: Savybės aprašymas.\n",
    "        \"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            self.df_property_type = self.pd.concat([\n",
    "                self.df_property_type,\n",
    "                self.pd.DataFrame({\"property_id\": [property_id], \"description\": [description]})\n",
    "            ], ignore_index=True).drop_duplicates(subset=[\"property_id\"])\n",
    "        elif self.engine == \"pyspark\":\n",
    "            new_row = self.spark.createDataFrame([(property_id, description)], schema=\"property_id STRING, description STRING\")\n",
    "            self.df_property_type = self.df_property_type.union(new_row)\n",
    "\n",
    "    def add_property(self, id: str, property_id: str, value: str, check_property_type: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Pridedama savybė konkrečiam objektui.\n",
    "\n",
    "        :param id: Objekto ID.\n",
    "        :param property_id: Savybės ID.\n",
    "        :param value: Savybės reikšmė.\n",
    "        :param check_property_type: Tikrinti, ar savybės tipas egzistuoja.\n",
    "        \"\"\"\n",
    "        if self.engine == \"pandas\":\n",
    "            if check_property_type and property_id not in self.df_property_type[\"property_id\"].values:\n",
    "                raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            self.df_property = self.pd.concat([\n",
    "                self.df_property,\n",
    "                self.pd.DataFrame({\"id\": [id], \"property_id\": [property_id], \"value\": [value]})\n",
    "            ], ignore_index=True).drop_duplicates(subset=[\"id\", \"property_id\"])\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if check_property_type:\n",
    "                existing = self.df_property_type.filter(f\"property_id = '{property_id}'\").count() > 0\n",
    "                if not existing:\n",
    "                    raise ValueError(f\"Savybės ID '{property_id}' nėra savybių tipų lentelėje.\")\n",
    "            new_row = self.spark.createDataFrame([(id, property_id, value)], schema=\"id STRING, property_id STRING, value STRING\")\n",
    "            self.df_property = self.df_property.union(new_row)\n",
    "\n",
    "    def import_from_wide(self, filepath: str, file_format: str, id_column: str = \"id\") -> None:\n",
    "        \"\"\"\n",
    "        Importuoja duomenis iš plačios lentelės.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather'.\n",
    "        :param id_column: Objekto ID stulpelio pavadinimas.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                wide_df = self.pd.read_csv(filepath)\n",
    "            elif file_format == \"parquet\":\n",
    "                wide_df = self.pd.read_parquet(filepath)\n",
    "            elif file_format == \"feather\":\n",
    "                wide_df = self.pd.read_feather(filepath)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su Pandas.\")\n",
    "            narrow_df = wide_df.melt(id_vars=id_column, var_name=\"property_id\", value_name=\"value\")\n",
    "            self.df_property = self.pd.concat([self.df_property, narrow_df], ignore_index=True).drop_duplicates()\n",
    "            rows = len(narrow_df)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                wide_df = self.spark.read.format(file_format).load(filepath)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su PySpark.\")\n",
    "            narrow_df = wide_df.selectExpr(f\"{id_column} AS id\", \"stack(*) AS (property_id, value)\")\n",
    "            self.df_property = self.df_property.union(narrow_df)\n",
    "            rows = narrow_df.count()\n",
    "        print(f\"Importuota {rows} eilučių iš {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def export_to_wide(self, filepath: str, file_format: str, id_column: str = \"id\") -> None:\n",
    "        \"\"\"\n",
    "        Eksportuoja duomenis į plačią lentelę.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather'.\n",
    "        :param id_column: Objekto ID stulpelio pavadinimas.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            wide_df = self.df_property.pivot(index=id_column, columns=\"property_id\", values=\"value\").reset_index()\n",
    "            if file_format == \"csv\":\n",
    "                wide_df.to_csv(filepath, index=False)\n",
    "            elif file_format == \"parquet\":\n",
    "                wide_df.to_parquet(filepath, index=False)\n",
    "            elif file_format == \"feather\":\n",
    "                wide_df.to_feather(filepath)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su Pandas\")\n",
    "            rows = len(wide_df)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            raise NotImplementedError(\"PySpark nepalaiko plačių lentelių eksporto.\")\n",
    "        print(f\"Eksportuota {rows} eilučių į {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def import_from_narrow(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Importuoja duomenis iš siauros lentelės.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                new_data = self.pd.read_csv(filepath)\n",
    "            elif file_format == \"parquet\":\n",
    "                new_data = self.pd.read_parquet(filepath)\n",
    "            elif file_format == \"feather\":\n",
    "                new_data = self.pd.read_feather(filepath)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su Pandas\")\n",
    "            self.df_property = self.pd.concat([self.df_property, new_data], ignore_index=True).drop_duplicates()\n",
    "            rows = len(new_data)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                new_data = self.spark.read.format(file_format).load(filepath)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su PySpark.\")\n",
    "            self.df_property = self.df_property.union(new_data)\n",
    "            rows = new_data.count()\n",
    "        print(f\"Importuota {rows} eilučių iš {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def export_to_narrow(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Eksportuoja duomenis į siaurą lentelę.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                self.df_property.to_csv(filepath, index=False)\n",
    "            elif file_format == \"parquet\":\n",
    "                self.df_property.to_parquet(filepath, index=False)\n",
    "            elif file_format == \"feather\":\n",
    "                self.df_property.to_feather(filepath)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su Pandas.\")\n",
    "            rows = len(self.df_property)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                self.df_property.write.mode(\"overwrite\").format(file_format).save(filepath)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su PySpark.\")\n",
    "            rows = self.df_property.count()\n",
    "        print(f\"Eksportuota {rows} eilučių į {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def import_from_file(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Importuoja duomenis iš failo.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather', 'sqlite'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                self.df_property = self.pd.read_csv(filepath)\n",
    "            elif file_format == \"parquet\":\n",
    "                self.df_property = self.pd.read_parquet(filepath)\n",
    "            elif file_format == \"feather\":\n",
    "                self.df_property = self.pd.read_feather(filepath)\n",
    "            elif file_format == \"sqlite\":\n",
    "                import sqlite3\n",
    "                with sqlite3.connect(filepath) as conn:\n",
    "                    self.df_property = self.pd.read_sql(f\"SELECT * FROM {self.name}_property\", conn)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su Pandas.\")\n",
    "            rows = len(self.df_property)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                self.df_property = self.spark.read.format(file_format).load(filepath)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su PySpark.\")\n",
    "            rows = self.df_property.count()\n",
    "        print(f\"Importuota {rows} eilučių iš {filepath}. Trukmė: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def export_to_file(self, filepath: str, file_format: str) -> None:\n",
    "        \"\"\"\n",
    "        Eksportuoja duomenis į failą.\n",
    "\n",
    "        :param filepath: Failo kelias.\n",
    "        :param file_format: Failo formatas: 'csv', 'parquet', 'feather', 'sqlite'.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.engine == \"pandas\":\n",
    "            if file_format == \"csv\":\n",
    "                self.df_property.to_csv(filepath, index=False)\n",
    "            elif file_format == \"parquet\":\n",
    "                self.df_property.to_parquet(filepath, index=False)\n",
    "            elif file_format == \"feather\":\n",
    "                self.df_property.to_feather(filepath)\n",
    "            elif file_format == \"sqlite\":\n",
    "                import sqlite3\n",
    "                with sqlite3.connect(filepath) as conn:\n",
    "                    self.df_property.to_sql(f\"{self.name}_property\", conn, if_exists=\"replace\", index=False)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su Pandas.\")\n",
    "            rows = len(self.df_property)\n",
    "        elif self.engine == \"pyspark\":\n",
    "            if file_format in [\"csv\", \"parquet\"]:\n",
    "                self.df_property.write.mode(\"overwrite\").format(file_format).save(filepath)\n",
    "            else:\n",
    "                raise ValueError(f\"Failo formatas '{file_format}' nepalaikomas su PySpark.\")\n",
    "            rows = self.df_property.count()\n",
    "        print(f\"Eksportuota {rows} eilučių į {filepath}. Trukmė: {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Uždaroma PySpark sesija, jei naudojama.\"\"\"\n",
    "        if self.engine == \"pyspark\":\n",
    "            self.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testuojame Pandas variklį su wide ir narrow lentelėmis\n",
      "Eksportuota 1000 eilučių į pandas_test_wide.csv. Trukmė: 0.02s\n",
      "Eksportuota 2874 eilučių į pandas_test_narrow.csv. Trukmė: 0.00s\n",
      "Importuota 26000 eilučių iš pandas_test_wide.csv. Trukmė: 0.03s\n",
      "Importuota 2874 eilučių iš pandas_test_narrow.csv. Trukmė: 0.02s\n",
      "Eksportuota 1000 eilučių į pandas_test_wide.parquet. Trukmė: 0.05s\n",
      "Eksportuota 26000 eilučių į pandas_test_narrow.parquet. Trukmė: 0.01s\n",
      "Importuota 26000 eilučių iš pandas_test_wide.parquet. Trukmė: 0.05s\n",
      "Importuota 26000 eilučių iš pandas_test_narrow.parquet. Trukmė: 0.02s\n",
      "Eksportuota 1000 eilučių į pandas_test_wide.feather. Trukmė: 0.03s\n",
      "Eksportuota 26000 eilučių į pandas_test_narrow.feather. Trukmė: 0.01s\n",
      "Importuota 26000 eilučių iš pandas_test_wide.feather. Trukmė: 0.04s\n",
      "Importuota 26000 eilučių iš pandas_test_narrow.feather. Trukmė: 0.02s\n",
      "Testuojame PySpark variklį tik su narrow lentelėmis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 19:06:53 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.1.91 instead (on interface wlo1)\n",
      "24/12/07 19:06:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/07 19:07:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/07 19:07:39 WARN DAGScheduler: Broadcasting large task binary with size 1575.3 KiB\n",
      "24/12/07 19:09:03 WARN DAGScheduler: Broadcasting large task binary with size 1168.4 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eksportuota 300 eilučių į pyspark_test_narrow.csv. Trukmė: 144.32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importuota 300 eilučių iš pyspark_test_narrow.csv. Trukmė: 9.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 19:10:06 WARN DAGScheduler: Broadcasting large task binary with size 1581.0 KiB\n",
      "24/12/07 19:11:08 WARN DAGScheduler: Broadcasting large task binary with size 1174.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eksportuota 600 eilučių į pyspark_test_narrow.parquet. Trukmė: 111.78s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importuota 600 eilučių iš pyspark_test_narrow.parquet. Trukmė: 1.97s\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    import string\n",
    "    import os\n",
    "\n",
    "    # NATO fonetinės abėcėlės sąrašas\n",
    "    nato_phonetic_alphabet = [\n",
    "        \"Alpha\", \"Bravo\", \"Charlie\", \"Delta\", \"Echo\", \n",
    "        \"Foxtrot\", \"Golf\", \"Hotel\", \"India\", \"Juliett\", \n",
    "        \"Kilo\", \"Lima\", \"Mike\", \"November\", \"Oscar\", \"Papa\", \n",
    "        \"Quebec\", \"Romeo\", \"Sierra\", \"Tango\", \"Uniform\", \n",
    "        \"Victor\", \"Whiskey\", \"X-ray\", \"Yankee\", \"Zulu\"\n",
    "    ]\n",
    "\n",
    "    # Testuojame Pandas variklį\n",
    "    print(\"Testuojame Pandas variklį su wide ir narrow lentelėmis\")\n",
    "    pandas_obj = properties_of(\"pandas_test_objects\", engine=\"pandas\")\n",
    "\n",
    "    # Įrašomos savybės\n",
    "    for phonetic in nato_phonetic_alphabet:\n",
    "        pandas_obj.add_property_type(property_id=phonetic.lower(), description=f\"Savybė {phonetic}\")\n",
    "\n",
    "    # Generuojami 1000 objektų su atsitiktinėmis savybėmis\n",
    "    num_objects = 1000\n",
    "    for i in range(1, num_objects + 1):\n",
    "        object_id = f\"obj_{i}\"\n",
    "        for _ in range(3):  # Trijų savybių priskyrimas\n",
    "            property_id = random.choice(nato_phonetic_alphabet).lower()\n",
    "            value = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n",
    "            pandas_obj.add_property(object_id, property_id, value)\n",
    "\n",
    "    # Eksportuojami ir importuojami duomenys Pandas\n",
    "    pandas_formats = [\"csv\", \"parquet\", \"feather\"]\n",
    "    for fmt in pandas_formats:\n",
    "        wide_filepath = f\"pandas_test_wide.{fmt}\"\n",
    "        narrow_filepath = f\"pandas_test_narrow.{fmt}\"\n",
    "\n",
    "        # Eksportuojame wide ir narrow\n",
    "        pandas_obj.export_to_wide(wide_filepath, file_format=fmt)\n",
    "        pandas_obj.export_to_narrow(narrow_filepath, file_format=fmt)\n",
    "\n",
    "        # Importuojame wide ir narrow\n",
    "        pandas_obj.import_from_wide(wide_filepath, file_format=fmt)\n",
    "        pandas_obj.import_from_narrow(narrow_filepath, file_format=fmt)\n",
    "\n",
    "    pandas_obj.close()\n",
    "\n",
    "    # Testuojame PySpark variklį\n",
    "    num_objects = 100\n",
    "    print(\"Testuojame PySpark variklį tik su narrow lentelėmis\")\n",
    "    pyspark_obj = properties_of(\"pyspark_test_objects\", engine=\"pyspark\")\n",
    "\n",
    "    # Įrašomos savybės\n",
    "    for phonetic in nato_phonetic_alphabet:\n",
    "        pyspark_obj.add_property_type(property_id=phonetic.lower(), description=f\"Savybė {phonetic}\")\n",
    "\n",
    "    # Generuojami 1000 objektų su atsitiktinėmis savybėmis\n",
    "    for i in range(1, num_objects + 1):\n",
    "        object_id = f\"obj_{i}\"\n",
    "        for _ in range(3):  # Trijų savybių priskyrimas\n",
    "            property_id = random.choice(nato_phonetic_alphabet).lower()\n",
    "            value = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n",
    "            pyspark_obj.add_property(object_id, property_id, value)\n",
    "\n",
    "    # Eksportuojami ir importuojami duomenys PySpark\n",
    "    pyspark_formats = [\"csv\", \"parquet\"]\n",
    "    for fmt in pyspark_formats:\n",
    "        narrow_filepath = f\"pyspark_test_narrow.{fmt}\"\n",
    "\n",
    "        # Eksportuojame narrow\n",
    "        pyspark_obj.export_to_narrow(narrow_filepath, file_format=fmt)\n",
    "\n",
    "        # Importuojame narrow\n",
    "        pyspark_obj.import_from_narrow(narrow_filepath, file_format=fmt)\n",
    "\n",
    "    pyspark_obj.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
